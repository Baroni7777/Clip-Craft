import os
import shutil
import time
import json
import re
import requests
from urllib.parse import quote, urlparse
from dotenv import load_dotenv
import uuid
from utils.video_helpers import *
from moviepy.config import change_settings
import google.generativeai as genai
import google.cloud.texttospeech as tts
import logging as log

load_dotenv()
change_settings(
    {"IMAGEMAGICK_BINARY": r"C:\\Program Files\\ImageMagick-7.1.1-Q16-HDRI\\magick.exe"}
)
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

PEXEL_HEADERS = {"Authorization": os.getenv("PEXEL_API_KEY")}
VID_PREF = "https://api.pexels.com/videos/search?"
IMG_PREF = "https://api.pexels.com/v1/search?"
SAVED_VIDEO_FORMAT = ".mp4"

with open("constants/example.json", "r") as file:
    example = json.load(file)

with open("config/config.json", "r") as file:
    config = json.load(file)

SYSTEM_MESSAGE = f"""
You are an AI content creator. You have been tasked to provide narration to video contents.

The user will provide the title, description, content type and duration of the final video. 
Attach is a list of media files, with descriptions generated by AI based on the media content.

You have to generate a script for each clip, and feel free to rearrange clips to form a coherent content.
You have integration with Pexel API and can add stock footage in between to make the content more engaging.
You must use all the media files provided by the user. And include some stock footage in between clips.

Choose the best transition that fits in well between clips, and you may insert text on-scene if it helps.
Always choose a suitable background music for the video.

As your response, write in this structured JSON format: \n {example}
Note the values above is just an example, Enclose property names in double quotes.
Here are some input options you can use: \n {config}
"""


video_model = genai.GenerativeModel(
    "gemini-1.5-flash", system_instruction=SYSTEM_MESSAGE
)
desc_model = genai.GenerativeModel("gemini-1.5-flash")


def upload_img(img_path):
    img_file = genai.upload_file(img_path)
    img_file = genai.get_file(img_file.name)
    return img_file


def upload_vid(vid_path):
    vid_file = genai.upload_file(vid_path)
    while vid_file.state.name == "PROCESSING":
        print(".", end="")
        time.sleep(2)
        vid_file = genai.get_file(vid_file.name)

    if vid_file.state.name == "FAILED":
        raise ValueError(vid_file.state.name)
    else:
        return vid_file


def query_pexel(url):
    response = requests.get(url, headers=PEXEL_HEADERS)
    response.raise_for_status()

    return response.json()


def download_stock(media_url, user_media_path: str):
    response = requests.get(media_url)
    response.raise_for_status()

    parsed_url = urlparse(media_url)
    filename = os.path.basename(parsed_url.path)
    file_path = os.path.join(f"{user_media_path}\\media", filename)

    with open(file_path, "wb") as file:
        file.write(response.content)

    return file_path


def text_to_speech(text, out_name,  user_media_path: str):
    text_input = tts.SynthesisInput(text=text)
    voice_params = tts.VoiceSelectionParams(
        language_code="en-US", name="en-US-Studio-O"
    )
    audio_config = tts.AudioConfig(audio_encoding=tts.AudioEncoding.LINEAR16)

    client = tts.TextToSpeechClient(
        client_options={"api_key": os.getenv("GCLOUD_API_KEY")}
    )
    response = client.synthesize_speech(
        input=text_input,
        voice=voice_params,
        audio_config=audio_config,
    )

    file_path = os.path.join(f"{user_media_path}\\audio", f"{out_name}.wav")
    with open(file_path, "wb") as out:
        out.write(response.audio_content)
        print(f'Generated speech saved to "{file_path}"')

    return file_path


def format_json(raw):
    match = re.search(r"```json(.*?)```", raw, re.DOTALL)
    if match:
        text = match.group(1).strip()
        return json.loads(text)
    return None


def start_script_generation(user_video_options: dict, DATABASE_OPERATIONS_SERVICE: any):
    
    title = user_video_options["title"]
    desc = user_video_options["description"]
    style = user_video_options["template"]
    duration = user_video_options["duration"]
    orientation = user_video_options["orientation"]
    unique_folder_id = user_video_options["user_media_path"]
    uploaded_files_names = user_video_options["uploaded_files_names"]

    user_media_path = f"temp\\{unique_folder_id}"

    if not title or not desc or not duration or not style or not orientation:
        return;
    else:
        if not os.path.exists(f"{user_media_path}\\media"):
            os.makedirs(f"{user_media_path}\\media")
            
        if not os.path.exists(f"{user_media_path}\\audio"):
            os.makedirs(f"{user_media_path}\\audio")

        if orientation == "portrait":
            res = (720, 1280)
        else:
            res = (1280, 720)

        
        media_data = []
        log.info("Analysing user media files")
        for file_name in uploaded_files_names:
            file_path = os.path.join(f"{user_media_path}\\media\\", file_name)
            file_obj = None
            prompt = None
            if file_path.endswith(".jpg"):
                prompt = "write a short one-sentence description for the image"
                file_obj = upload_img(file_path)
            elif file_path.endswith(".mp4"):
                prompt = "write a short one-paragraph description for the video, depending on its duration."
                file_obj = upload_vid(file_path)

            log.info(f"Generating description for {file_name}")
            response = desc_model.generate_content([file_obj, prompt])
            genai.delete_file(file_obj.name)

            media_data.append({"source": file_path, "desc": response.text})
            

        video_prompt = f"""title: {title} description: {desc} style: {style} duration: {duration}
        Media clips and AI descriptions: {media_data}
        """
        response = video_model.generate_content(video_prompt)
        log.info(response.text)

        script = format_json(response.text)
        scenes = script["scenes"];
        
        # Downloading stock footage
        log.info("Downloading stock footage")
        for clip in script["scenes"]:
            source, form = clip["type"].split("_")
            if source == "stock":
                suffix = f"query={quote(clip['query'])}&per_page=1"

                if form == "video":
                    response = query_pexel(VID_PREF + suffix)
                    media_url = response["videos"][0]["video_files"][0]["link"]
                    file_path = download_stock(media_url, user_media_path=user_media_path)

                elif form == "photo":
                    response = query_pexel(IMG_PREF + suffix)
                    media_url = response["photos"][0]["src"]["landscape"]
                    file_path = download_stock(media_url, user_media_path=user_media_path)

                clip["media_path"] = file_path
                del clip["query"]



        # Generating Narration
        log.info("Generating Narration")
        for clip in script["scenes"]:
            file_name = os.path.splitext(os.path.basename(clip["media_path"]))[0]
            text = clip["script"]
            audio_path = text_to_speech(text, file_name, user_media_path=user_media_path)
            clip["audio_path"] = audio_path
            del clip["script"]


        clips = []
        
        music_file = os.path.join("music", script["music"] + ".mp3")
        for i, pair in enumerate(script["scenes"]):
            form = pair["type"].split("_")[1]
            if form == "photo":
                clip = create_photo_clip(pair["media_path"], pair["audio_path"], res)
            elif form == "video":
                clip = create_video_clip(pair["media_path"], pair["audio_path"], res)

            if pair["text_overlay"]:
                clip = add_text_overlay(clip, pair["text_overlay"])

            # if i > 0:
            #     prev_clip = clips[-1]
            #     transition_type = script["scenes"][i - 1].get("transition", "fade")
            #     prev_clip, clip = VideoTransitionHelper.apply_transition(
            #         prev_clip, clip, transition_type
            #     )
            #     clips[-1] = prev_clip

            clips.append(clip)

        final_clip = concatenate_videoclips(clips)
        final_clip = add_background_music(final_clip, music_file)
        final_video_path = f"{user_media_path}\\final_video.mp4"
        
        final_clip.write_videofile(
            final_video_path, codec="libx264", audio_codec="aac"
        )

        unique_final_video_name = str(uuid.uuid4())+SAVED_VIDEO_FORMAT
        DATABASE_OPERATIONS_SERVICE.upload_file_by_path(final_video_path, unique_final_video_name)
        signed_file_url = DATABASE_OPERATIONS_SERVICE.get_file_link(key=unique_final_video_name)
        shutil.rmtree(f"{user_media_path}\\media", ignore_errors=True)
        shutil.rmtree(f"{user_media_path}\\audio", ignore_errors=True)
        shutil.rmtree(final_video_path, ignore_errors=True)
        return {"signed_url":signed_file_url, "scenes":scenes}


